import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import random

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import (  # –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
    make_scorer, roc_curve, precision_recall_curve,
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
)
from collections import Counter

# –ó–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º —Å–∏–¥—ã
SEED = 127
np.random.seed(SEED)
random.seed(SEED)


def find_optimal_threshold(y_true, y_proba):
    """
    –ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
    :param y_true: –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
    :param y_proba: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤
    :return: –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
    """
    n_samples = len(y_true)  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
    P = sum(y_true)  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ "1"
    N = n_samples - P  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ "0"
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
    # –í—ã—á–∏—Å–ª—è–µ–º F1-score –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ä–æ–≥–∞
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)
    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ä–æ–≥ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º F1
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    return optimal_threshold


def find_optimal_threshold_by_accuracy(y_true, y_proba):
    """
    –ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –ø–æ –º–µ—Ç—Ä–∏–∫–µ accuracy.
    :param y_true: –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ (0 –∏–ª–∏ 1)
    :param y_proba: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
    :return: –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
    """
    thresholds = np.linspace(0, 1, 10_000)  # –ø—Ä–æ–±—É–µ–º 10_000 –ø–æ—Ä–æ–≥–æ–≤ –æ—Ç 0 –¥–æ 1
    best_accuracy = 0
    best_threshold = 0.5  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    for threshold in thresholds:
        y_pred = (y_proba >= threshold).astype(int)
        acc = accuracy_score(y_true, y_pred)
        if acc > best_accuracy:
            best_accuracy = acc
            best_threshold = threshold

    return best_threshold


def get_classification_metrics(y_true, y_pred, y_proba=None):
    """
    –†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
    :param y_true: –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (0 –∏–ª–∏ 1)
    :param y_pred: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (0 –∏–ª–∏ 1)
    :param y_proba: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (–ø–æ –∂–µ–ª–∞–Ω–∏—é)
    :return: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'f1': f1_score(y_true, y_pred, zero_division=0),
    }

    if y_proba is not None:
        # y_proba –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        metrics['roc_auc'] = roc_auc_score(y_true, y_proba)

    return metrics


def train_valid_model(model_class, model_num, model_params, df_train, df_valid,
                      model_cols, target_col):
    """
    –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
    :param model_class: –ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    :param model_num: –ü–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä –º–æ–¥–µ–ª–∏
    :param model_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    :param df_train: –æ–±—É—á–∞—é—â–∏–π –î–§
    :param df_valid: –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –î–§
    :param model_cols: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    :param target_col: —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    :return: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –î–§ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    X_train, y_train = df_train[model_cols], df_train[target_col]
    X_valid, y_valid = df_valid[model_cols], df_valid[target_col]

    if model_class.__name__ == 'GradientBoosting':
        model = model_class(**model_params)
        model.fit(X_train, y_train, eval_sets=[{'X': X_valid, 'y': y_valid}, ])
    else:
        model = model_class(**model_params, random_state=SEED)
        model.fit(X_train, y_train)

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    if hasattr(model, 'predict_proba'):
        y_train_proba = model.predict_proba(X_train)[:, 1]
        y_valid_proba = model.predict_proba(X_valid)[:, 1]
    else:
        y_train_proba = model.predict(X_train)
        y_valid_proba = model.predict(X_valid)

    # –ü–æ–¥–±–∏—Ä–∞–µ–º –ø–æ—Ä–æ–≥ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
    optimal_threshold = find_optimal_threshold(y_valid, y_valid_proba)
    # optimal_threshold = find_optimal_threshold_by_accuracy(y_valid, y_valid_proba)
    print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {optimal_threshold:.4f}")  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: 0.2336
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥ –∫ –æ–±–µ–∏–º –≤—ã–±–æ—Ä–∫–∞–º
    y_train_pred = (y_train_proba >= optimal_threshold).astype(int)
    y_valid_pred = (y_valid_proba >= optimal_threshold).astype(int)

    metrics_train = get_classification_metrics(y_train, y_train_pred, y_train_proba)
    metrics_valid = get_classification_metrics(y_valid, y_valid_pred, y_valid_proba)

    metric_t = f'Train{model_num}'
    metric_v = f'Valid{model_num}'

    # –°–æ–∑–¥–∞–µ–º DataFrame
    metrics = pd.DataFrame({'Metric': list(metrics_train.keys()),
                            metric_t: list(metrics_train.values()),
                            metric_v: list(metrics_valid.values())})

    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É train –∏ valid (–≤ %)
    metrics[f'Diff{model_num},%'] = ((metrics[metric_v] - metrics[metric_t])
                                     / metrics[metric_t] * 100).round(2)
    return model, metrics, optimal_threshold


def find_best_model(metrics_df):
    """
    –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –ø–æ–∏—Å–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
    :param metrics_df: –î–§ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    :return: None
    """
    models = []
    for idx, row in metrics_df.iterrows():
        metric_name = row['Metric']

        # –û—Ç–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫–æ–ª–æ–Ω–æ–∫ Valid*
        valid_cols = [col for col in metrics_df.columns if col.startswith('Valid')]
        valid_values = row[valid_cols]

        best_col = valid_values.idxmax()
        best_val = valid_values.max()

        models.append(best_col)

        print(f"–ú–µ—Ç—Ä–∏–∫–∞: {metric_name:<10} --> –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_col} ({best_val:.4f})")

    result = Counter(models).most_common()[0]
    print('\n–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {} –Ω–∞ {} –º–µ—Ç—Ä–∏–∫–∞—Ö –∏–∑ {}'.format(*result, len(models)))


def plot_feature_importance(model, feature_names, name_model='CatBoost', top_n=20):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–æ–¥–µ–ª–∏.
    :param model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    :param feature_names: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
    :param name_model: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    :param top_n: –°–∫–æ–ª—å–∫–æ —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å
    """
    # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    # importances = model.get_feature_importance()
    importances = model.feature_importances_

    # –°–æ–∑–¥–∞—ë–º DataFrame
    feat_imp_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values('Importance', ascending=False).reset_index(drop=True)

    # –û—Ç—Ä–∏—Å–æ–≤–∫–∞
    plt.figure(figsize=(12, 6))
    sns.barplot(x='Importance', y='Feature', data=feat_imp_df.head(top_n), palette='viridis')
    plt.title(f'{name_model} ‚Äî –¢–æ–ø {top_n} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏')
    plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
    plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫')
    plt.tight_layout()
    plt.show()
    return feat_imp_df  # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π


def find_depth(model_class, model_params, train, valid, model_columns, target, depths=None):
    """
    –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –¥–µ—Ä–µ–≤—å–µ–≤
    :param model_class: –ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    :param model_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    :param train: –æ–±—É—á–∞—é—â–∏–π –î–§
    :param valid: –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –î–§
    :param model_columns: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    :param target: —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    :param depths: –¥–∏–∞–ø–∞–∑–æ–Ω –ø–æ–∏—Å–∫–∞ –≥–ª—É–±–∏–Ω—ã –¥–µ—Ä–µ–≤–∞
    :return: –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞
    """
    if depths is None:
        depths = range(3, 21)

    train_f1, valid_f1 = [], []
    for depth in depths:

        model_params['max_depth'] = depth

        _, metrics, _ = train_valid_model(model_class, 0, model_params,
                                          train, valid, model_columns, target)

        # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        t_f1, v_f1 = metrics[metrics['Metric'] == 'f1'][['Train0', 'Valid0']].values[0]
        train_f1.append(t_f1)
        valid_f1.append(v_f1)

        print(f"Depth: {depth:2} | Train F1: {t_f1:.5f} | Valid F1: {v_f1:.5f}")

        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞, –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π F1 < –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ,
        # –∏ –º–∏–Ω–∏–º—É–º –±—ã–ª —Ö–æ—Ç—è –±—ã 2 —à–∞–≥–∞ –Ω–∞–∑–∞–¥
        max_valid_f1 = max(valid_f1)
        max_index = valid_f1.index(max_valid_f1)
        if len(valid_f1) > max_index + 3 and valid_f1[-1] < max_valid_f1:
            print(f"–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: –ª—É—á—à–∏–π Valid F1 ({max_valid_f1:.5f}) "
                  f"–±—ã–ª –Ω–∞ –≥–ª—É–±–∏–Ω–µ {depths[max_index]}")
            break

    plt.figure(figsize=(10, 6))
    plt.plot(depths[:len(train_f1)], train_f1, label='Train F1', marker='o')
    plt.plot(depths[:len(valid_f1)], valid_f1, label='Valid F1', marker='o')
    plt.xlabel('Max Depth')
    plt.ylabel('F1')
    plt.title('–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å F1 –æ—Ç –≥–ª—É–±–∏–Ω—ã –¥–µ—Ä–µ–≤–∞')
    plt.legend()
    plt.grid(True)
    plt.show()

    return depths[max_index]


def make_submit(model, X_test, threshold, reverse_mapping, postfix='', save_to_excel=False):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∞–π–ª–∞ —Å–∞–±–º–∏—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –ö–∞–≥–≥–ª
    :param model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    :param X_test: –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    :param threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    :param reverse_mapping: —Ä–µ–≤–µ—Ä—Å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    :param postfix: –ø–æ—Å—Ç—Ñ–∏–∫—Å –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ —Å–∞–±–º–∏—Ç–∞
    :param save_to_excel: —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å X_test –≤ —ç–∫—Å–µ–ª—å
    :return: –ò–º—è —Ñ–∞–π–ª–∞ —Å–∞–±–º–∏—Ç–∞
    """
    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    target = 'Personality'
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
    file_submit = f'submit_{model.__class__.__name__}{postfix}.csv'
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    if hasattr(model, 'predict_proba'):
        y_test_proba = model.predict_proba(X_test)[:, 1]
    else:
        y_test_proba = model.predict(X_test)
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –∫–ª–∞—Å—Å—ã 0 / 1
    y_test_pred = (y_test_proba >= threshold).astype(int)
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
    X_test[target] = y_test_pred
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
    X_test[target] = X_test[target].map(reverse_mapping)
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    X_test[target].to_csv(file_submit)
    if save_to_excel:
        X_test.to_excel(file_submit.replace('.csv', '.xlsx'))
    print(f'–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª —Å–∞–±–º–∏—Ç–∞: {file_submit}')
    return file_submit


def set_types(df, num_cols, cat_cols):
    # –Ø–≤–Ω–æ –∑–∞–¥–∞–µ–º —Ç–∏–ø—ã
    for col in num_cols:
        df[col] = df[col].astype(int)
    for col in cat_cols:
        df[col] = df[col].astype('category')
    return df


class DataTransform:
    def __init__(self, numeric_columns=None, category_columns=None, set_category=False,
                 features2drop=None, set_num_int=True, preprocessor=None, **kwargs):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        :param numeric_columns: —Ü–∏—Ñ—Ä–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        :param category_columns: –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        :param set_category: —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∫–∞–∫ "category"
        :param features2drop: –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å
        :param set_num_int: –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–∏–ø INT
        :param preprocessor: –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤
        :param kwargs: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        """
        self.set_num_int = set_num_int
        self.set_category = set_category
        self.category_columns = [] if category_columns is None else category_columns
        self.numeric_columns = [] if numeric_columns is None else numeric_columns
        self.features2drop = [] if features2drop is None else features2drop
        self.preprocessor = KNNImputer if preprocessor is None else preprocessor
        self.prep_kwargs = kwargs if kwargs else dict(n_neighbors=7)
        self.p_imputer = None
        self.cat_cols = self.category_columns.copy()
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        self.target = 'Personality'
        # –ö–æ–ª–æ–Ω–∫–∏: —á–∏—Å–ª–æ–≤—ã–µ + –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
        self.model_columns = []
        # –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤
        self.imputer_cols = []
        # –ö–æ–ª–æ–Ω–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
        self.columns_with_nans = []
        self.columns_with_missing = []
        # –ö–æ–ª–æ–Ω–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ –º–æ–¥–µ–ª–∏
        self.all_features = []
        # –ò—Å—Ö–æ–¥–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        self.mapping_target = {'Extrovert': 0, 'Introvert': 1}
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–≤–µ—Ä—Å–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
        self.reverse_mapping = {v: k for k, v in self.mapping_target.items()}
        # –°–ª–æ–≤–∞—Ä—å –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.mapping_yes_no = {'Yes': 1, 'No': 0, 'nan': 2}
        # –°–ª–æ–≤–∞—Ä—å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–æ–∫ –ø–æ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        self.grp_stats = {}
        self.grp_stats_cols = []

    def preprocess_data(self, df, fill_nan_cat=False):
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        :param sample: –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
        :param fill_nan_cat: –∑–∞–ø–æ–ª–Ω—è—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–µ–º 'nan'
        :return: –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
        """
        for col in self.cat_cols:
            if fill_nan_cat:
                # –ó–∞–ø–æ–ª–Ω–∏–º –ø—Ä–æ–ø—É—Å–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–µ–º 'nan'
                df[col] = df[col].astype(str).fillna('nan')
            df[col] = df[col].map(self.mapping_yes_no)
        # –ó–∞–∫–æ–¥–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        if self.target in df.columns:
            df[self.target] = df[self.target].map(self.mapping_target).astype(int)
        return df

    def make_attribute_columns(self, df):
        """
        –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø–∏—Å–∫–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∏ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        :param df: –î–§
        :return: —Å–ø–∏—Å–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∏ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        """
        # –ö–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å
        features2drop = self.features2drop + [self.target]

        # –í—ã–±–∏—Ä–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–≤–∫–ª—é—á–∞—è —Å—Ç—Ä–æ–∫–∏ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
        category_columns = (df.drop(columns=features2drop, errors='ignore')
                            .select_dtypes(include=['object', 'category'])
                            .columns.tolist())

        # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        numeric_columns = (df.drop(columns=features2drop, errors='ignore')
                           .select_dtypes(include=['number'])
                           .columns.tolist())

        # –ö–æ–ª–æ–Ω–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ –º–æ–¥–µ–ª–∏
        self.model_columns = numeric_columns + category_columns
        return category_columns, numeric_columns

    def set_category_cols(self, df):
        if self.set_category:
            # –í–µ—Ä–Ω–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            for col in self.cat_cols:
                df[col] = df[col].astype('category')
        else:
            for col in self.cat_cols:
                df[col] = df[col].astype(int)
        return df

    @staticmethod
    def apply_group_stats(df, group_stats, global_mean, feature_cols):
        for col in feature_cols:
            new_col = f"{col}_group_mean"
            df[new_col] = df[col].map(group_stats[col])
            df[new_col].fillna(global_mean, inplace=True)
        return df

    def fit(self, df, fill_nan_cat=False, add_new_features=False):
        """
        –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏—á
        :param df: –∏—Å—Ö–æ–¥–Ω—ã–π –§–î
        :param fill_nan_cat: –∑–∞–ø–æ–ª–Ω—è—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–µ–º 'nan'
        :param add_new_features: –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        :return: –î–§ —Å –∞–≥—Ä–µ–≥–∞—Ü–∏—è–º–∏
        """
        df = df.copy()

        # –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø–∏—Å–∫–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∏ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        category_columns, numeric_columns = self.make_attribute_columns(df)

        # –µ—Å–ª–∏ –Ω–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ --> –∑–∞–ø–æ–ª–Ω–∏–º –∏—Ö
        if not self.category_columns:
            self.category_columns = category_columns.copy()
            self.cat_cols = category_columns.copy()

        # –µ—Å–ª–∏ –Ω–µ—Ç —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ --> –∑–∞–ø–æ–ª–Ω–∏–º –∏—Ö
        if not self.numeric_columns:
            self.numeric_columns = numeric_columns.copy()

        self.columns_with_nans = []
        self.columns_with_missing = []
        for col in df.columns:
            if df[col].isnull().any() and col != 'Pers_orig':
                self.columns_with_nans.append(col)
                self.columns_with_missing.append(f"{col}_nan")

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = self.preprocess_data(df.copy(), fill_nan_cat=fill_nan_cat)

        self.imputer_cols = self.model_columns.copy()

        # –§–æ—Ä–º–∏—Ä—É–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        self.grp_stats_cols = []
        for col in self.imputer_cols:
            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ int
            temp = df[[col, self.target]].dropna(subset=[col]).copy()
            temp[col] = temp[col].astype(int)
            # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.grp_stats[col] = temp.groupby(col)[self.target].mean().to_dict()
            self.grp_stats_cols.append(f"{col}_tar_mean")

        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Imputer
        self.p_imputer = self.preprocessor(**self.prep_kwargs)
        self.p_imputer.fit(df[self.imputer_cols])

        if add_new_features:
            # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
            if self.set_num_int:
                df[self.imputer_cols] = (self.p_imputer.transform(df[self.imputer_cols])
                                         .round()
                                         .astype(int)
                                         )
            else:
                df[self.imputer_cols] = self.p_imputer.transform(df[self.imputer_cols])

            # –í–µ—Ä–Ω–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            df = self.set_category_cols(df)

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            df = self.add_new_features(df)

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–æ–≤ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
            pass

            # –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø–∏—Å–∫–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∏ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            self.category_columns, self.numeric_columns = self.make_attribute_columns(df)

    def transform(self, df, fill_nan_cat=False, add_grp_target=False, add_new_features=False):
        """
        –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ñ–∏—á
        :param df: –î–§
        :param fill_nan_cat: –∑–∞–ø–æ–ª–Ω—è—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–µ–º 'nan'
        :param add_grp_target: –¥–æ–±–∞–≤–∏—Ç—å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        :param add_new_features: –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        :return: –î–§ —Å —Ñ–∏—á–∞–º–∏
        """
        df = df.copy()
        # –û—Ç–º–µ—Ç–∏–º —Å—Ç—Ä–æ–∫–∏ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
        for col, col_nan in zip(self.columns_with_nans, self.columns_with_missing):
            df[col_nan] = df[col].isnull().astype(int)

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = self.preprocess_data(df, fill_nan_cat=fill_nan_cat)

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        if self.set_num_int:
            df[self.imputer_cols] = (self.p_imputer.transform(df[self.imputer_cols])
                                     .round()
                                     .astype(int)
                                     )
        else:
            df[self.imputer_cols] = self.p_imputer.transform(df[self.imputer_cols])

        if add_grp_target:
            for col in self.imputer_cols:
                new_col = f"{col}_tar_mean"
                df[new_col] = df[col].map(self.grp_stats[col])
                # df[new_col].fillna(df[self.target].mean(), inplace=True)

            # print(df.isna().sum())

        # –í–µ—Ä–Ω–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df = self.set_category_cols(df)

        all_features_add = []
        if add_new_features:
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            df = self.add_new_features(df)
            all_features_add = df.drop(columns=self.target, errors='ignore').columns

        if not self.all_features:
            self.all_features = self.model_columns + self.columns_with_missing
            self.all_features.extend([col for col in all_features_add
                                      if col not in self.all_features])
            if add_grp_target:
                self.all_features.extend([col for col in self.grp_stats_cols
                                          if col not in self.all_features])

        model_columns = self.all_features.copy()
        if self.target in df.columns:
            model_columns.append(self.target)

        # –û—Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ –Ω—É–∂–Ω–æ–º –Ω–∞–º –ø–æ—Ä—è–¥–∫–µ
        return df[model_columns]

    def fit_transform(self, df, fill_nan_cat=False, add_grp_target=False,
                      add_new_features=False):
        """
        Fit + transform data
        :param df: –∏—Å—Ö–æ–¥–Ω—ã–π –§–î
        :param fill_nan_cat: –∑–∞–ø–æ–ª–Ω—è—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–µ–º 'nan'
        :param add_grp_target: –¥–æ–±–∞–≤–∏—Ç—å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        :param add_new_features: –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        :return: –î–§ —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        self.fit(df, fill_nan_cat=fill_nan_cat, add_new_features=add_new_features)
        df = self.transform(df, fill_nan_cat=fill_nan_cat, add_grp_target=add_grp_target,
                            add_new_features=add_new_features)
        return df

    @staticmethod
    def drop_constant_columns(df):
        # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        col_to_drop = []
        for col in df.columns:
            if df[col].nunique() == 1:
                col_to_drop.append(col)
        if col_to_drop:
            df.drop(columns=col_to_drop, inplace=True)
        return df

    @staticmethod
    def add_new_features(df: pd.DataFrame) -> pd.DataFrame:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        :param df: –∏—Å—Ö–æ–¥–Ω—ã–π –î–§
        :return: –î–§ —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        df = df.copy()

        # 1. üìä –ë–∏–Ω–Ω–∏–Ω–≥–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df['alone_bin'] = pd.cut(
            df['Time_spent_Alone'],
            bins=[-1, 2, 4, 11],
            labels=['low', 'medium', 'high']
        )  # –ú–∞–ª–æ / —Å—Ä–µ–¥–Ω–µ / –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –≤ –æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–µ

        df['friends_bin'] = pd.cut(
            df['Friends_circle_size'],
            bins=[-1, 5, 10, 15],
            labels=['few', 'medium', 'many']
        )  # –†–∞–∑–º–µ—Ä –∫—Ä—É–≥–∞ –æ–±—â–µ–Ω–∏—è

        df['outside_bin'] = pd.cut(
            df['Going_outside'],
            bins=[-1, 3, 5, 7],
            labels=['homebody', 'balanced', 'outgoing']
        )  # –ß–∞—Å—Ç–æ—Ç–∞ –≤—ã—Ö–æ–¥–∞ –∏–∑ –¥–æ–º–∞

        df['posts_bin'] = pd.cut(
            df['Post_frequency'],
            bins=[-1, 3, 6, 10],
            labels=['inactive', 'moderate', 'active']
        )  # –ß–∞—Å—Ç–æ—Ç–∞ –ø–æ—Å—Ç–∏–Ω–≥–∞

        df['events_bin'] = pd.cut(
            df['Social_event_attendance'],
            bins=[-1, 3, 6, 10],
            labels=['rare', 'moderate', 'frequent']
        )  # –ß–∞—Å—Ç–æ—Ç–∞ —É—á–∞—Å—Ç–∏—è –≤ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è—Ö

        # 2. üß† –ò–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏

        # –°–æ—Ü–∏–∞–ª—å–Ω–∞—è –∏–∑–æ–ª—è—Ü–∏—è
        df['loneliness_index'] = df['Time_spent_Alone'] / (df['Friends_circle_size'] + 1)

        # –û–±—â–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–Ω–µ –¥–æ–º–∞
        df['social_activity'] = df['Social_event_attendance'] + df['Going_outside']

        # –ò–Ω–¥–µ–∫—Å –∏–Ω—Ç—Ä–æ–≤–µ—Ä—Å–∏–∏ (–µ—Å–ª–∏ —É—Å—Ç–∞–ª–æ—Å—Ç—å –æ—Ç –æ–±—â–µ–Ω–∏—è ‚Äî +3 –±–∞–ª–ª–∞)
        df['introvert_score'] = (df['Time_spent_Alone'] +
                                 df['Drained_after_socializing'].astype(int) * 3)

        # –ß–∞—Å—Ç–æ—Ç–∞ –ø–æ—Å—Ç–æ–≤ –Ω–∞ –æ–¥–Ω–æ–≥–æ –¥—Ä—É–≥–∞
        df['post_per_friend'] = df['Post_frequency'] / (df['Friends_circle_size'] + 1)

        # –ë–∞–ª–∞–Ω—Å –æ—Ñ—Ñ–ª–∞–π–Ω/–æ–Ω–ª–∞–π–Ω –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        df['event_vs_post_ratio'] = df['Social_event_attendance'] / (df['Post_frequency'] + 1)

        # –ù–∞—Å–∫–æ–ª—å–∫–æ —á–µ–ª–æ–≤–µ–∫ –∞–∫—Ç–∏–≤–µ–Ω –∏ –Ω–µ —É—Å—Ç–∞—ë—Ç –æ—Ç –æ–±—â–µ—Å—Ç–≤–∞
        df['active_life_index'] = df['Going_outside'] * (
                1 - df['Drained_after_socializing'].astype(int))

        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π —Ç—Ä–µ–≤–æ–∂–Ω–æ—Å—Ç–∏ (–æ–±–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞ = 1)
        df['social_anxiety'] = (df['Stage_fear'].astype(int) &
                                df['Drained_after_socializing'].astype(int)).astype(int)

        # 3. üîç –ü—Ä–∏–∑–Ω–∞–∫ "–µ—Å—Ç—å –ª–∏ –ø—Ä–æ–ø—É—Å–∫–∏ –≤–æ–æ–±—â–µ"
        nan_cols = [col for col in df.columns if col.endswith('_nan')]
        df['has_any_missing'] = df[nan_cols].sum(axis=1).gt(0).astype(int)

        return df


def make_train_valid(test_size=0.2, return_full_df=False, add_original_df=False,
                     round_to_int=False):
    """
    –§—É–Ω–∫—Ü–∏—è —á—Ç–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏
    :param test_size: —Ä–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π —á–∞—Å—Ç–∏
    :param return_full_df: –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–Ω—ã–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –î–§
    :param add_original_df: –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–∫–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –î–§
    :param round_to_int: –æ–∫—Ä—É–≥–ª–∏—Ç—å float –¥–æ int
    :return: train, valid, test
    """
    # –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv('train.csv')
    test = pd.read_csv('test.csv')

    if add_original_df:
        merge_cols = test.drop(columns='id').columns.to_list()
        # –ó–∞–≥—Ä—É–∑–∏–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –î–§ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ç–æ—Ä–æ–≥–æ –±—ã–ª–∏ —Å–¥–µ–ª–∞–Ω—ã train.csv –∏ test.csv
        df_orig = (pd.read_csv('personality_datasert.csv')
                   .rename(columns={'Personality': 'Pers_orig'})
                   .fillna(-99)
                   .drop_duplicates(subset=merge_cols)
                   )
        if round_to_int:
            # –û–∫—Ä—É–≥–ª—è–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ NaN –¥–æ —Ü–µ–ª–æ–≥–æ
            for col in df_orig.select_dtypes(include=['number']):
                df_orig[col] = df_orig[col].round()
        # –ó–∞–∫–æ–¥–∏—Ä—É–µ–º 'Pers_orig' –∫–∞–∫ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        mapping_target = {'Extrovert': 0, 'Introvert': 1}
        df_orig['Pers_orig'] = df_orig['Pers_orig'].map(mapping_target).astype(int)
        # –î–æ–±–∞–≤–∏–º –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –≤ –æ–±–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        df = df.fillna(-99).merge(df_orig, on=merge_cols, how='left').replace(-99, np.nan)
        test = test.fillna(-99).merge(df_orig, on=merge_cols, how='left').replace(-99, np.nan)
        df = df.merge(df_orig, on=merge_cols, how='left')
        test = test.merge(df_orig, on=merge_cols, how='left')

    # –ö–æ–ª–æ–Ω–∫–∞ "id" –Ω–µ –Ω–µ—Å–µ—Ç —Å–º—ã—Å–ª–∞ - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å
    df.set_index("id", inplace=True)
    test.set_index("id", inplace=True)

    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    target = 'Personality'

    # –¢.–∫. —É –Ω–∞—Å –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: –∫–ª–∞—Å—Å —Å –º–µ—Ç–∫–æ–π "1" –≤—Å–µ–≥–æ 26%
    # –ë—É–¥–µ–º –¥–µ–ª–∏—Ç—å —Å–æ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
    train, valid = train_test_split(df, test_size=test_size, stratify=df[target],
                                    random_state=SEED)
    if return_full_df:
        return train, valid, test, df

    return train, valid, test


def add_group_stats_transform(df: pd.DataFrame, train_stats: dict = None) -> tuple[
    pd.DataFrame, dict]:
    df = df.copy()

    bin_cols = ['alone_bin', 'friends_bin', 'events_bin', 'outside_bin', 'posts_bin']
    features = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside',
                'Friends_circle_size', 'Post_frequency']

    # 1. –ì—Ä—É–ø–ø–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–ø–æ –±–∏–Ω–∞–º)
    for bin_col in bin_cols:
        for col in features:
            grp = df.groupby(bin_col)[col]
            df[f'{bin_col}_{col}_mean'] = grp.transform('mean')
            df[f'{bin_col}_{col}_std'] = grp.transform('std')

    # 2. Z-–æ—Ü–µ–Ω–∫–∏ (–≥–ª–æ–±–∞–ª—å–Ω—ã–µ ‚Äî –ø–æ –≤—Å–µ–º—É train)

    stats = {} if train_stats is None else train_stats  # —Å–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ä–µ–¥–Ω–∏–º–∏/—Å—Ç–¥

    for col in features:
        if train_stats is None:
            mean = df[col].mean()
            std = df[col].std()
            stats[col] = (mean, std)
        else:
            mean, std = stats[col]

        df[f'{col}_zscore'] = (df[col] - mean) / std

    return df, stats


def compute_group_stats(df_train: pd.DataFrame) -> dict:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–∏–µ –∏ std –∑–Ω–∞—á–µ–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞–∂–¥–æ–π –±–∏–Ω-–≥—Ä—É–ø–ø–µ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å DataFrame'–æ–≤ —Å –∞–≥—Ä–µ–≥–∞—Ç–∞–º–∏ –ø–æ –≥—Ä—É–ø–ø–∞–º:
    """
    stats = {}
    target_cols = ['Post_frequency', 'Time_spent_Alone', 'Going_outside',
                   'Social_event_attendance']

    bin_cols = ['alone_bin', 'friends_bin', 'events_bin', 'outside_bin', 'posts_bin']

    for bin_col in bin_cols:
        group_stat = df_train.groupby(bin_col)[target_cols].agg(['mean', 'std']).reset_index()
        # Flatten MultiIndex
        group_stat.columns = [f"{bin_col}_{col[0]}_{col[1]}" if col[1] else col[0] for col in
                              group_stat.columns]
        stats[bin_col] = group_stat

    return stats


def apply_group_stats(df: pd.DataFrame, stats: dict) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏/—Ç–µ—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä—É–ø–ø –∏–∑ train:
    :param df:
    :param stats:
    :return:
    """
    df = df.copy()
    for bin_col, group_df in stats.items():
        df = df.merge(group_df, how='left', left_on=bin_col, right_on=group_df.columns[0])
    return df


if __name__ == '__main__':
    train, valid, test, df = make_train_valid(return_full_df=True, add_original_df=True,
                                              round_to_int=False)

    print(train.shape, valid.shape, test.shape)
