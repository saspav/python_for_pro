import random
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import shap

from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

import optuna
from optuna.integration import LightGBMPruningCallback
from lightgbm import LGBMClassifier, Dataset, cv

import warnings

warnings.filterwarnings("ignore")

from some_functions_clf import (SEED, make_train_valid, DataTransform, find_depth,
                                train_valid_model, make_submit)

def objective(trial):
    # boosting_type = trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss'])
    boosting_type = 'gbdt'

    params_cv = {
        'boosting_type': boosting_type,
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),
        'num_leaves': trial.suggest_int('num_leaves', 10, 100),
        'max_depth': trial.suggest_int('max_depth', 3, 5),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 1.0, log=True),
        'random_state': SEED,
        'n_jobs': -1,
        'verbosity': -1,
        'feature_pre_filter': False,  # üí° –∫—Ä–∏—Ç–∏—á–Ω–æ!
    }

    # GBDT –∏ DART –∏—Å–ø–æ–ª—å–∑—É—é—Ç bagging
    if boosting_type != 'goss':
        params_cv['subsample'] = trial.suggest_float('subsample', 0.6, 1.0)
        params_cv['bagging_fraction'] = trial.suggest_float('bagging_fraction', 0.6, 1.0)
        params_cv['bagging_freq'] = trial.suggest_int('bagging_freq', 1, 7)
    else:
        params_cv['subsample'] = 1.0  # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏

    # DART-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    if boosting_type == 'dart':
        params_cv['drop_rate'] = trial.suggest_float('drop_rate', 0.1, 0.5)
        params_cv['skip_drop'] = trial.suggest_float('skip_drop', 0.0, 0.5)
        params_cv['max_drop'] = trial.suggest_int('max_drop', 10, 100)
        params_cv['uniform_drop'] = trial.suggest_categorical('uniform_drop', [True, False])

    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π –∏ pruning
    cv_result = cv(
        params=params_cv,
        train_set=dtrain,
        num_boost_round=500,
        nfold=3,
        stratified=True,
        # early_stopping_rounds=50,
        metrics='auc',
        seed=SEED,
        callbacks=[LightGBMPruningCallback(trial, 'auc')],
        # verbose_eval=False
    )
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π AUC
    return np.mean(cv_result['valid auc-mean'])


# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ Pandas
pd.set_option('display.max_columns', 50)  # –û—Ç–æ–±—Ä–∞–∂–∞—Ç—å –¥–æ 50 —Å—Ç–æ–ª–±—Ü–æ–≤
pd.set_option('display.precision', 5)  # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –î–§ —Å 5-—é –∑–Ω–∞–∫–∞–º–∏ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π

# –ó–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º —Å–∏–¥—ã
np.random.seed(SEED)
random.seed(SEED)

# –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
target = 'Personality'

train, valid, test = make_train_valid()

dts = DataTransform(set_category=True, preprocessor=KNNImputer, n_neighbors=5)

# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
train = dts.fit_transform(train)
valid = dts.transform(valid)

# –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
model_columns = dts.all_features
cat_cols = dts.category_columns

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
params = {'categorical_feature': cat_cols, 'force_row_wise': True,
          'n_jobs': -1, 'verbosity': -1}

xb1, metrics_df5, _ = train_valid_model(LGBMClassifier, 'LGC', params,
                                        train, valid, model_columns, target)

print(metrics_df5, '\n')
metrics_df = metrics_df5

# # –ü–æ–¥–±–µ—Ä–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –≥–ª—É–±–∏–Ω—É –¥–µ—Ä–µ–≤–∞
# opt_depth_xb = find_depth(LGBMClassifier, params, train, valid, model_columns, target)
# # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: –ª—É—á—à–∏–π Valid F1 (0.94136) –±—ã–ª –Ω–∞ –≥–ª—É–±–∏–Ω–µ 6
# exit()

X_train, y_train = train[model_columns], train[target]
X_valid, y_valid = valid[model_columns], valid[target]

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ DMatrix
dtrain = Dataset(X_train, y_train, categorical_feature=cat_cols, free_raw_data=False)
dvalid = Dataset(X_valid, y_valid, categorical_feature=cat_cols, free_raw_data=False)
# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ –≤—ã–≤–æ–¥–æ–≤
optuna.logging.set_verbosity(optuna.logging.WARNING)

# # –°–æ–∑–¥–∞–µ–º study —Å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º maximize
# study = optuna.create_study(direction='maximize')
# study.optimize(objective, n_trials=200, show_progress_bar=True)
#
# # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# print(f"–õ—É—á—à–∏–π AUC: {study.best_value:.4f}")  # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è AUC
# print("–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:", study.best_params)

# lg_best_grid = {'boosting_type': 'goss', 'learning_rate': 0.010527356559607244,
#                 'num_leaves': 54, 'max_depth': 4, 'min_child_samples': 26,
#                 'colsample_bytree': 0.6498369482631124, 'reg_alpha': 0.001996825899468912,
#                 'reg_lambda': 0.0029056516001497996,
#                 'n_jobs': -1, 'verbosity': -1}

lg_best_grid = {'learning_rate': 0.012168409321350424, 'num_leaves': 78, 'max_depth': 4,
                'min_child_samples': 31, 'colsample_bytree': 0.7152350985965117,
                'reg_alpha': 0.0152465652290482, 'reg_lambda': 0.0026314520377431084,
                'subsample': 0.7475755007546926, 'bagging_fraction': 0.6356741993284825,
                'bagging_freq': 6,
                'n_jobs': -1, 'verbosity': -1}

lg2, metrics_df6, optimal_threshold = train_valid_model(LGBMClassifier, 6, lg_best_grid,
                                                        train, valid, model_columns, target)

metrics_df = pd.concat([metrics_df, metrics_df6.drop(columns=['Metric'])], axis=1)
print(metrics_df)

# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
test = dts.transform(test)

optimal_threshold = 0.5

# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∞–±–º–∏—Ç–∞: –ø–µ—Ä–µ–¥–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
_ = make_submit(lg2, test[model_columns], optimal_threshold, dts.reverse_mapping)

# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: 0.4264 (max_depth = 4)
#       Metric  TrainLGC  ValidLGC  DiffLGC,%   Train6   Valid6  Diff6,%
# 0   accuracy   0.97173   0.97004      -0.17  0.96896  0.96869    -0.03
# 1  precision   0.94538   0.95042       0.53  0.94271  0.94921     0.69
# 2     recall   0.94611   0.93368      -1.31  0.93782  0.92953    -0.88
# 3         f1   0.94575   0.94198      -0.40  0.94026  0.93927    -0.11
# 4    roc_auc   0.99309   0.96473      -2.86  0.97151  0.96841    -0.32

# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: 0.2792 (max_depth = 5)
#       Metric  TrainLGC  ValidLGC  DiffLGC,%   Train6   Valid6  Diff6,%
# 0   accuracy   0.97173   0.97004      -0.17  0.96896  0.96896     0.00
# 1  precision   0.94538   0.95042       0.53  0.94225  0.94926     0.74
# 2     recall   0.94611   0.93368      -1.31  0.93834  0.93057    -0.83
# 3         f1   0.94575   0.94198      -0.40  0.94029  0.93982    -0.05
# 4    roc_auc   0.99309   0.96473      -2.86  0.97544  0.96955    -0.60

# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: 0.3020 (max_depth = 4 - 'gbdt')
#       Metric  TrainLGC  ValidLGC  DiffLGC,%   Train6   Valid6  Diff6,%
# 0   accuracy   0.97173   0.97004      -0.17  0.96903  0.96869    -0.03
# 1  precision   0.94538   0.95042       0.53  0.94295  0.94921     0.66
# 2     recall   0.94611   0.93368      -1.31  0.93782  0.92953    -0.88
# 3         f1   0.94575   0.94198      -0.40  0.94038  0.93927    -0.12
# 4    roc_auc   0.99309   0.96473      -2.86  0.97425  0.96932    -0.51